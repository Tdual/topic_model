{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook\n"
     ]
    }
   ],
   "source": [
    "from urllib import request \n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        print(\"notebook\")\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "except (NameError, RuntimeError):\n",
    "    from tqdm import tqdm\n",
    "import re\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n",
    "stopwords = [line.decode(\"utf-8\").strip() for line in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, pos=False):\n",
    "    text = re.sub(r\"http://.*\\.com\", \"\", text)\n",
    "    text = re.sub(r\"【.*?】\", \"\",text)\n",
    "    text = text.lower()\n",
    "    l = [line.split(\"\\t\") for line in mecab.parse(text).split(\"\\n\")]\n",
    "    res = [i[0] if not pos else (i[0],i[3]) for i in l \n",
    "                   if len(i) >=4 \n",
    "                       and (\"名詞\" in i[3] or \"動詞\" in i[3] or \"形容詞\" in i[3] )\n",
    "                       and \"数\" not in i[3] and \"助動詞\" not in i[3] and \"助詞\" not in [3]\n",
    "                       and  not re.search(\"[0-9]\", i[0])\n",
    "                       and i[0] not in stopwords\n",
    "            ]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['認め', '自分自身', '若さ故の過ち']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"認めたくないものだな。自分自身の若さ故の過ちというものを。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('text/movie-enter'),\n",
       " PosixPath('text/it-life-hack'),\n",
       " PosixPath('text/kaden-channel'),\n",
       " PosixPath('text/topic-news'),\n",
       " PosixPath('text/livedoor-homme'),\n",
       " PosixPath('text/peachy'),\n",
       " PosixPath('text/sports-watch'),\n",
       " PosixPath('text/dokujo-tsushin'),\n",
       " PosixPath('text/smax')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_path = \"./text/\"\n",
    "doc_dir = Path(doc_path)\n",
    "dirs = [i for i in doc_dir.iterdir() if i.is_dir()]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [a for categ in dirs for a in categ.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7376"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(doc_id):\n",
    "    with articles[doc_id].open() as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc_manager():\n",
    "    def __init__(self, docs):\n",
    "        self.docs = docs\n",
    "        \n",
    "    def read_doc(self, doc_id):\n",
    "        with self.docs[doc_id].open() as f:\n",
    "            print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = Doc_manager(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.livedoor.com/article/detail/5978741/\n",
      "2011-10-30T10:15:00+0900\n",
      "【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か\n",
      "　2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による初の長編サスペンス『八日目の蝉』。2010年に檀れいと北乃きいの出演によりテレビドラマ化された同作が、2011年4月に永作博美と井上真央の出演によって映画化。そして、劇場公開から半年が過ぎた10月28日、DVD＆ブルーレイとなって発売されました。\n",
      "\n",
      "八日目の蝉\n",
      "　妻子ある男と愛し合い、その子を身ごもりながら、あきらめざるをえなかった女。彼女は同時に、男の妻が子供を産んだことを知る。その赤ん坊を見に行った女は、突発的にその子を連れ去り、逃避行を続けた挙句、小豆島に落ち着き、母と娘として暮らしはじめる。\n",
      "\n",
      "\n",
      "不倫相手の子供を誘拐し、4年間育てた女\n",
      "　永作博美が演じる野々宮希和子は、不倫相手の子を宿しながらも、彼の「いずれ妻と別れるから、それまで待ってくれ」という常套句を信じて、中絶。後遺症により、二度と子供を産めない身体となってしまいます。その後、不倫相手から彼の妻が出産したことを知らされ、別れを決意。最後に諦めをつけるため、彼らの生後6ヶ月の赤ん坊・恵理菜の顔を見た希和子でしたが、自分に笑顔で向けた恵理菜を見て、思わず誘拐。名前を変えて恵理菜を薫と名付けると、人目を避けて各地を転々とし、二人で幸せな時間を過ごしますが、辿り着いた最後の場所・小豆島で4年の逃避行に終止符を打ちます。\n",
      "\n",
      "\n",
      "誘拐犯に育てられた女\n",
      "　4歳になって実の両親と再会を果たした後も、世間から言われの無い中傷を受け、本当の両親との関係を築けないまま、21歳の大学生へと成長した恵理菜。過去と向き合うことを避けてきた恵理菜でしたが、劇団ひとりが演じる不倫相手・岸田孝史の子を宿し、ずっと憎み続けてきた希和子と同じ道を歩んでいることに気付いた彼女は、小池栄子が演じるルポライター・安藤千草と共に、4年間の逃亡生活を追憶する旅に出ます。希和子との幸せだった時間に触れながら、最終地・小豆島に辿り着いた恵理菜が見た真実とは？\n",
      "\n",
      "\n",
      "八日目の蝉は幸せなのだろうか？\n",
      "　蝉は俗説として、一生の大半を幼虫として地下で費やし、地上に出て羽化からわずか1週間程度で死ぬと言われています。八日目まで生き残ってしまった蝉が目にしたのは、孤独か、あるいは誰も目にすることのできなかった世界なのでしょうか。中絶によって二度と子供を埋めない身体になった希和子が、恵理菜と過ごした4年間の“八日目”は、希和子だけでなく、恵理菜にとっても幸せな時間だったのではないでしょうか。\n",
      "\n",
      "　主題歌は、デビュー10年目に突入した2010年10月より、耳管開放症のため音楽活動を休止していた中島美嘉の復帰第一弾（通算33作目）シングル『Dear』。原作に深い感銘を受けた彼女が、本作のために提供した楽曲となっています。\n",
      "\n",
      "・八日目の蝉 - 作品情報\n",
      "\n",
      "八日目の蝉　通常版 [DVD]posted with amazlet at 11.10.29アミューズソフトエンタテインメント (2011-10-28)\n",
      "売り上げランキング: 266\n",
      "Amazon.co.jp で詳細を見る\n",
      "■関連記事\n",
      "・井上真央が逃避行？永作、森口、小池も頭を悩ます映画とは\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dm.read_doc(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ad6391bca64ae0a009566730d9b027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for a in tqdm(articles):\n",
    "    with a.open() as f:\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        docs.append(tokenizer(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(w for d in docs for w in d)\n",
    "word2id = dict(zip(vocab, itertools.count()))\n",
    "id2word = dict(list(enumerate(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "num_words = 0 \n",
    "for d in docs:\n",
    "    num_words += len(d)\n",
    "    corpus.append([word2id[w] for w in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14765"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = corpus[:20]\n",
    "train_corpus = corpus[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(train_corpus)\n",
    "#M = len(corpus)\n",
    "V =  len(vocab)\n",
    "k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 4994 8\n"
     ]
    }
   ],
   "source": [
    "print(M,V,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_ = np.random.rand(M, k)\n",
    "lambda_ = np.random.rand(V, k)\n",
    "q_ = np.random.rand(M,V,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 8)\n",
      "(4994, 8)\n",
      "(30, 4994, 8)\n"
     ]
    }
   ],
   "source": [
    "print(gamma_.shape)\n",
    "print(lambda_.shape)\n",
    "print(q_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per(corpus, alpha, eta, n_itr=100):\n",
    "    perplexity = 0.0\n",
    "    N = 0\n",
    "    for d in corpus:\n",
    "        N += len(d)\n",
    "    for _ in range(n_itr):\n",
    "        theta = np.array([np.random.dirichlet(a) for a in alpha])\n",
    "        beta = np.array([np.random.dirichlet(e) for e in eta.T])\n",
    "        m = np.inner(theta, beta.T)\n",
    "        log_p = 0.0\n",
    "        for i, d in enumerate(corpus):\n",
    "            log_p += np.log(m[i][d]).sum()\n",
    "        perplexity += np.exp(-log_p/N)\n",
    "    perplexity = perplexity/n_itr\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7533.3591627705446"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = gamma_\n",
    "eta = lambda_\n",
    "get_per(test_corpus, alpha, eta, n_itr=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ec800e210f47d98950fe14b4151b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/tamoto.yoshifumi/anaconda2/envs/py36/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  6287.26917582\n",
      "1 :  6074.75949955\n",
      "2 :  5955.33258383\n",
      "3 :  5872.23197668\n",
      "4 :  5799.02512798\n",
      "5 :  5707.68683936\n",
      "6 :  5633.09988448\n",
      "7 :  5545.13460617\n",
      "8 :  5465.60170856\n",
      "9 :  5402.40116459\n",
      "10 :  5311.30989983\n",
      "11 :  5224.97043116\n",
      "12 :  5146.70921357\n",
      "13 :  5070.61974821\n",
      "14 :  4981.45065034\n"
     ]
    }
   ],
   "source": [
    "for itr in tqdm(range(100)):\n",
    "    \n",
    "    for d in range(M):\n",
    "        doc = train_corpus[d]\n",
    "        N_d = len(doc)\n",
    "        for n in range(N_d):\n",
    "            gamma_sum = gamma_.sum(axis=1)\n",
    "            lambda_sum = lambda_.sum(axis=0)\n",
    "            w = int(doc[n])\n",
    "\n",
    "            q_[d,w] = np.exp(digamma(gamma_[d]) - digamma(gamma_sum)[d] + digamma(lambda_[w]) - digamma(lambda_sum))\n",
    "            q_ = q_/q_.sum()\n",
    "            gamma_[d] += q_.sum(axis=1)[d]\n",
    "            lambda_[w] += q_.sum(axis=0)[w]\n",
    "            \n",
    "            #for i in range(k):\n",
    "            #    gamma_di = gamma_[d, i]\n",
    "            #    lambda_wi = lambda_[wl, i]\n",
    "             #   print(digamma(gamma_sum).shape)\n",
    "             #   q_[d,w,i] = np.exp(digamma(gamma_di) - digamma(gamma_sum)[d] + digamma(lambda_wi) - digamma(lambda_sum)[i])\n",
    "             #   gamma_[d,i] += q_.sum(axis=1)[d,i]\n",
    "             #   lambda_[w,i] += q_.sum(axis=0)[w,i]\n",
    "                \n",
    "    alpha = gamma_ - q_.sum(axis=1)\n",
    "    eta = lambda_ - q_.sum(axis=0)\n",
    "    \n",
    "    perplexity = get_per(test_corpus, alpha, eta, n_itr=200)\n",
    "    print(itr, \": \", perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=4\n",
    "14 :  5338.01566202\n",
    "    \n",
    "k=5\n",
    "20 :  4814.45180145\n",
    "    \n",
    "k=6\n",
    "20 :  4735.64121761\n",
    "    \n",
    "k=7    \n",
    "20 :  4694.68933233\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(p_w.T[test_corpus[0]].T[0].prod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w.T[test_corpus[0]].T[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(\"# \", i)\n",
    "    for t in sorted([(i,rate) for (i,rate) in enumerate(beta.T[i])], key=lambda t: t[1],reverse=True)[:10]:\n",
    "        print(dictionary[t[0]],\" : \",t[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([np.random.dirichlet(a) for a in alpha])\n",
    "beta = np.array([np.random.dirichlet(e) for e in eta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w = np.inner(theta, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[3156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(np.inner(theta, beta)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normliation(q):\n",
    "    return q/q.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta = normliation(np.array([np.random.dirichlet(g) for g in gamma_]).prod(axis=0))\n",
    "q_beta = normliation(np.array([np.random.dirichlet(l) for l in lambda_]).prod(axis=1))\n",
    "q_z = q_[d,w,i].prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([np.random.dirichlet(g) for g in gamma_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def d_KL(p1,p2):\n",
    "    return stats.entropy(p1, p2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([[1/2,1/2],[1/2,1/2]])\n",
    "beta=  np.array([[1/3,1/3,1/3],[1/3,1/3,1/3]]).T\n",
    "test_corpus = [[1,2],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w = np.inner(theta, beta)\n",
    "print(p_w)\n",
    "log_p = 0.0\n",
    "N = 0\n",
    "for i, d in enumerate(test_corpus):\n",
    "    log_p += np.log(p_w.T[d].T[i].prod())\n",
    "    N += len(d)\n",
    "perplexity = np.exp(-log_p/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
